# Pipeline de Datalake con ETL orquestado por lotes en AWS

Este proyecto implementa un pipeline de datos automatizado (ETL) que simula la carga masiva y peri√≥dica de datos empresariales. Orquesta la ingesta, transformaci√≥n, catalogaci√≥n y notificaci√≥n de datos utilizando servicios nativos de AWS.

## üèõÔ∏è Arquitectura

El flujo de trabajo es orquestado totalmente por **AWS Step Functions** y sigue estos pasos:

1. **Ingesta:** Los datos crudos (CSV) se almacenan en S3.
2. **Trigger:** **EventBridge Scheduler** activa el pipeline autom√°ticamente (simulaci√≥n batch/nocturna).
3. **Procesamiento (ETL):** **AWS Glue** (Python Shell) limpia los datos y los convierte a formato **Parquet**.
4. **Catalogaci√≥n:** Un **Glue Crawler** actualiza el Data Catalog autom√°ticamente.
5. **Calidad y Alertas:** Manejo de errores con **SNS** (env√≠o de correos en caso de √âxito o Fallo).
6. **Anal√≠tica:** Consultas SQL con **Athena** y visualizaci√≥n en **QuickSight**.

![Diagrama de Arquitectura](architecture/diagrama_architecture.jpg)

## üõ†Ô∏è Servicios AWS utilizados

* **Orquestaci√≥n:** AWS Step Functions (Workflow Studio).
* **ETL:** AWS Glue (Jobs & Crawlers).
* **Almacenamiento:** Amazon S3 (Raw & Processed zones).
* **Consultas:** Amazon Athena.
* **Visualizaci√≥n:** Amazon QuickSight (Quick Suite).
* **Automatizaci√≥n:** Amazon EventBridge Scheduler.
* **Notificaciones:** Amazon SNS.

## üìÇ Estructura del repositorio

* `/src`: Contiene el script de transformaci√≥n en Python (PySpark/Python Shell).
* `/orchestration`: Definici√≥n JSON (ASL) de la State Machine.
* `/sql`: Consultas de validaci√≥n y an√°lisis usadas en Athena.

## üöÄ C√≥mo desplegar

1. **S3:** Crear bucket con carpetas `raw/`, `processed/` y `scripts/`.
2. **IAM:** Configurar roles con permisos para Glue, Step Functions y EventBridge.
3. **Glue:** Crear el Job copiando el script de `/src`.
4. **Step Functions:** Importar el c√≥digo de `/orchestration` en una nueva m√°quina de estado.
5. **EventBridge:** Configurar la regla cron para el disparo autom√°tico.

## üìä Resultados

### Ejecuci√≥n Orquestador (Step Functions)
![Orquestador](images/stepfunctions_graph.png)

### Dashboard de Negocio (QuickSight)
![Dashboard](./images/dashboard_quicksight.png)

# üìç Alcance del Proyecto y Suposiciones de Ingesta

Este proyecto se enfoca en el ciclo de vida del dato post-ingesta (Procesamiento, Orquestaci√≥n y Consumo).

Simulaci√≥n de la Fuente de Datos: Se asume la existencia de un sistema transaccional externo (ej. un ERP de Ventas o un Servidor de Logs) que exporta sus datos peri√≥dicamente.

Mecanismo Simulado: En un entorno productivo, un proceso automatizado (como un Cron Job o AWS Transfer Family) depositar√≠a archivos CSV en el bucket S3 (/raw) diariamente a una hora espec√≠fica.

En esta Demo: Esta ingesta se simula mediante la carga manual de archivos CSV al bucket S3, lo cual representa el lote de datos ("Batch") del d√≠a a procesar.

Disparador: El Pipeline detecta la presencia de estos datos o cumple su horario programado (EventBridge) para iniciar el ETL.

## üë• Autores

* Valeria Orozco Monsalve
* Lucas Osorno Ospina

---
*Proyecto realizado como parte del curso de Administraci√≥n de servidores y redes*
